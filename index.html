<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Dictaphone Whisper</title>
<style>
  body { font-family: sans-serif; padding: 20px; background: #0f172a; color: #e2e8f0; }
  button { padding: 12px 18px; border: none; border-radius: 10px; font-size: 16px; cursor: pointer; margin: 5px; }
  .start { background: #22c55e; color: #04110a; }
  .stop { background: #ef4444; color: #240808; }
  .save { background: #3b82f6; color: #061428; }
  #log { margin-top: 15px; padding: 10px; background: #1e293b; border-radius: 8px; min-height: 120px; white-space: pre-wrap; }
</style>
</head>
<body>
<h1>Dictaphone avec transcription Whisper</h1>

<button id="start" class="start">‚ñ∂Ô∏è Commencer l'enregistrement</button>
<button id="stop" class="stop" disabled>‚èπÔ∏è Arr√™ter & Transcrire</button>

<div id="log">En attente...</div>

<script>
let mediaRecorder, audioChunks = [];
const log = document.getElementById('log');
const startBtn = document.getElementById('start');
const stopBtn = document.getElementById('stop');

// Remplace par ta cl√© API
const OPENAI_API_KEY = "COLLE_TA_CLE_ICI";

startBtn.addEventListener('click', async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
    audioChunks = [];
    mediaRecorder.ondataavailable = e => { if (e.data.size > 0) audioChunks.push(e.data); };
    mediaRecorder.start();
    log.textContent = "üéôÔ∏è Enregistrement en cours...";
    startBtn.disabled = true;
    stopBtn.disabled = false;
  } catch (err) {
    log.textContent = "Erreur micro : " + err.message;
  }
});

stopBtn.addEventListener('click', async () => {
  mediaRecorder.stop();
  log.textContent = "‚è≥ Traitement...";
  stopBtn.disabled = true;
  startBtn.disabled = false;

  mediaRecorder.onstop = async () => {
    const blob = new Blob(audioChunks, { type: 'audio/webm' });
    const formData = new FormData();
    formData.append("file", blob, "audio.webm");
    formData.append("model", "whisper-1"); // mod√®le OpenAI
    formData.append("language", "fr");

    try {
      const res = await fetch("https://api.openai.com/v1/audio/transcriptions", {
        method: "POST",
        headers: { "Authorization": `Bearer ${OPENAI_API_KEY}` },
        body: formData
      });
      const data = await res.json();
      if (data.text) {
        log.textContent = "Texte transcrit :\n\n" + data.text;
      } else {
        log.textContent = "Erreur transcription : " + JSON.stringify(data);
      }
    } catch (e) {
      log.textContent = "Erreur r√©seau : " + e.message;
    }
  };
});
</script>
</body>
</html>
